{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time Series waether Forecasting\n",
    "1. single feat. predict based on diff features\n",
    "2. many to many\n",
    "3. one to one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time series library import\n",
    "import os         #it decides path from where get the data or save it\n",
    "import datetime \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf \n",
    "import plotly.express as px\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# managing figure size in the begining of the code\n",
    "mpl.rcParams['figure.figsize'] = (15,6)\n",
    "mpl.rcParams['axes.grid'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/jena_climate_2009_2016.csv.zip\n",
      "\u001b[1m13568290/13568290\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 1us/step\n"
     ]
    }
   ],
   "source": [
    "#import the weather dataset\n",
    "# import the dataset\n",
    "zip_path = tf.keras.utils.get_file(\n",
    "    origin = 'https://storage.googleapis.com/tensorflow/tf-keras-datasets/jena_climate_2009_2016.csv.zip',\n",
    "    fname = 'jena_climate_2009_2016.csv.zip',\n",
    "    extract = True)\n",
    "csv_path, _ = os.path.splitext(zip_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the data in pandas dataframe\n",
    "df = pd.read_csv(csv_path)\n",
    "# # write the csv file\n",
    "# df.to_csv('jena_climate_2009_2016.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "420551*15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print all the columns\n",
    "pd.set_option('display.max_rows', None)\n",
    "df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the data based on the Date Time column for every 2 hours interval\n",
    "# Slice [start:stop:step], starting from index 11 take every 12th record.\n",
    "df = df[11::12]\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_time = pd.to_datetime(df.pop('Date Time'), format='%d.%m.%Y %H:%M:%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's plot the data\n",
    "plot_cols = ['T (degC)', 'p (mbar)']\n",
    "plot_features = df[plot_cols]\n",
    "plot_features.index = date_time\n",
    "plot_features.plot(subplots=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets plot only the first 500 hours dataset which is equal to 250 records\n",
    "# let's plot the data\n",
    "plot_cols = ['T (degC)', 'p (mbar)']\n",
    "plot_features = df[plot_cols][:250]\n",
    "plot_features.index = date_time[:250]\n",
    "plot_features.plot(subplots=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets decompose the data into trend, seasonality and noise\n",
    "# we will use the seasonal_decompose function from statsmodels library\n",
    "decompose = seasonal_decompose(df['T (degC)'], model='additive', period=365)\n",
    "decompose.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decompose = seasonal_decompose(df['p (mbar)'], model='additive', period=365)\n",
    "decompose.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decompose = seasonal_decompose(df['rh (%)'], model='additive', period=365)\n",
    "decompose.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets plot the same plots in plotly\n",
    "fig = px.line(df, x=date_time, y='T (degC)')\n",
    "fig.show()\n",
    "fig = px.line(df, x=date_time, y='p (mbar)')\n",
    "fig.update_traces(line_color='red')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean up the data\n",
    "# lets check if there are any null values in the dataset\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv = df['wv (m/s)']\n",
    "bad_wv = wv == -9999.0  \n",
    "wv[bad_wv] = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_wv = df['max. wv (m/s)']\n",
    "bad_max_wv = max_wv == -9999.0\n",
    "max_wv[bad_max_wv] = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#modeling\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into train, validation and test i have 35045 records make 70% for training, 20% for validation and 10% for testing\n",
    "# 70% of 35045 is 24531\n",
    "# 20% of 35045 is 7009\n",
    "# 10% of 35045 is 3505\n",
    "# total 35045\n",
    "# train data\n",
    "train_df = df[:24531]\n",
    "# validation data\n",
    "val_df = df[24531:31540]\n",
    "# test data\n",
    "test_df = df[31540:]\n",
    "\n",
    "# lets check the shape of the data\n",
    "train_df.shape, val_df.shape, test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets plot the data\n",
    "train_df.plot(subplots=True)\n",
    "\n",
    "# lets plot the data\n",
    "val_df.plot(subplots=True)\n",
    "\n",
    "# lets plot the data\n",
    "test_df.plot(subplots=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets normalize the data\n",
    "# we will use the mean and standard deviation of the training data to normalize the data\n",
    "train_mean = train_df.mean()\n",
    "train_std = train_df.std()\n",
    "\n",
    "# lets normalize the training data\n",
    "train_df = (train_df - train_mean) / train_std\n",
    "# lets normalize the validation data\n",
    "val_df = (val_df - train_mean) / train_std\n",
    "# lets normalize the test data\n",
    "test_df = (test_df - train_mean) / train_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets plot the data\n",
    "train_df.plot(subplots=True)\n",
    "\n",
    "# lets plot the data\n",
    "val_df.plot(subplots=True)\n",
    "\n",
    "# lets plot the data\n",
    "test_df.plot(subplots=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets make the model to predict the temperature\n",
    "# lets make the windowed dataset\n",
    "# we will use the windowed dataset to train the model\n",
    "# we will use the windowed dataset to validate the model\n",
    "# we will use the windowed dataset to test the model\n",
    "# we will use the windowed dataset to predict the model\n",
    "# we will use the windowed dataset to plot the model\n",
    "\n",
    "# lets make the windowed datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
